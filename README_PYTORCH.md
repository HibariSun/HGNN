# PyTorchæ·±åº¦å­¦ä¹ è¶…å›¾ç¥ç»ç½‘ç»œ - snoRNA-Diseaseå…³è”é¢„æµ‹

## ğŸš€ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ªåŸºäºPyTorchçš„**å…ˆè¿›æ·±åº¦è¶…å›¾ç¥ç»ç½‘ç»œ**ï¼Œç”¨äºé¢„æµ‹snoRNAä¸ç–¾ç—…ä¹‹é—´çš„å…³è”å…³ç³»ã€‚è¯¥æ¨¡å‹é‡‡ç”¨äº†å¤šç§æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š

- âœ¨ **è¶…å›¾å·ç§¯å±‚** (Hypergraph Convolution)
- âœ¨ **å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶** (Multi-Head Attention)
- âœ¨ **æ®‹å·®è¿æ¥** (Residual Connection)
- âœ¨ **æ‰¹å½’ä¸€åŒ–** (Batch Normalization)
- âœ¨ **Dropoutæ­£åˆ™åŒ–**
- âœ¨ **å­¦ä¹ ç‡è°ƒåº¦** (Learning Rate Scheduling)
- âœ¨ **æ—©åœæœºåˆ¶** (Early Stopping)

---

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

### æœ€ä½è¦æ±‚
- Python 3.8+
- RAM: 8GB+
- å­˜å‚¨: 2GB+

### æ¨èé…ç½®
- Python 3.9+
- RAM: 16GB+
- GPU: NVIDIA GPU with CUDA support (å¯é€‰ï¼Œä½†æ¨èç”¨äºåŠ é€Ÿ)
- å­˜å‚¨: 5GB+

---

## ğŸ”§ å®‰è£…æŒ‡å—

### æ–¹æ³•1: CPUç‰ˆæœ¬ï¼ˆé€‚åˆå¤§å¤šæ•°ç”¨æˆ·ï¼‰

```bash
# å®‰è£…PyTorch (CPUç‰ˆæœ¬)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu

# å®‰è£…å…¶ä»–ä¾èµ–
pip install numpy pandas scikit-learn matplotlib seaborn tqdm
```

### æ–¹æ³•2: GPUç‰ˆæœ¬ï¼ˆéœ€è¦NVIDIA GPUï¼‰

```bash
# å®‰è£…PyTorch (CUDA 11.8ç‰ˆæœ¬)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install numpy pandas scikit-learn matplotlib seaborn tqdm
```

### æ–¹æ³•3: ä½¿ç”¨requirementsæ–‡ä»¶

```bash
pip install -r requirements_pytorch.txt
```

### éªŒè¯å®‰è£…

```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
```

---

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### 1. ç½‘ç»œç»“æ„

```
è¾“å…¥å±‚ (ç›¸ä¼¼åº¦çŸ©é˜µ)
    â†“
ç‰¹å¾æŠ•å½±å±‚ (Linear + BatchNorm + ELU + Dropout)
    â†“
è¶…å›¾å·ç§¯å— Ã— 3
â”‚   â”œâ”€ è¶…å›¾å·ç§¯
â”‚   â”œâ”€ å¤šå¤´æ³¨æ„åŠ›
â”‚   â”œâ”€ æ®‹å·®è¿æ¥
â”‚   â”œâ”€ å±‚å½’ä¸€åŒ–
â”‚   â””â”€ å‰é¦ˆç½‘ç»œ (FFN)
    â†“
å…¨å±€æ³¨æ„åŠ›æ± åŒ–
    â†“
é¢„æµ‹å¤´ (Linear Ã— 3 + Sigmoid)
    â†“
è¾“å‡º (å…³è”æ¦‚ç‡çŸ©é˜µ)
```

### 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### è¶…å›¾å·ç§¯å±‚ (HypergraphConvolution)
```python
# è¶…å›¾å·ç§¯å…¬å¼
X' = D_v^(-1/2) * H * D_e^(-1) * H^T * D_v^(-1/2) * X * W
```
- æ•´åˆèŠ‚ç‚¹çš„é«˜é˜¶å…³ç³»
- é€šè¿‡è¶…è¾¹ä¼ æ’­ä¿¡æ¯
- å½’ä¸€åŒ–ä¿è¯æ•°å€¼ç¨³å®šæ€§

#### å¤šå¤´æ³¨æ„åŠ› (MultiHeadAttention)
```python
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) * V
```
- 8ä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œè®¡ç®—
- æ•è·ä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯
- å¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›

#### è¶…å›¾å— (HypergraphBlock)
- ç»“åˆè¶…å›¾å·ç§¯å’Œæ³¨æ„åŠ›æœºåˆ¶
- ä½¿ç”¨æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- å±‚å½’ä¸€åŒ–åŠ é€Ÿæ”¶æ•›

### 3. æ¨¡å‹å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| hidden_dims | [256, 128, 64] | éšè—å±‚ç»´åº¦ |
| num_heads | 8 | æ³¨æ„åŠ›å¤´æ•° |
| dropout | 0.3 | Dropoutç‡ |
| learning_rate | 0.001 | åˆå§‹å­¦ä¹ ç‡ |
| weight_decay | 1e-5 | L2æ­£åˆ™åŒ–ç³»æ•° |
| batch_size | - | å…¨æ‰¹æ¬¡è®­ç»ƒ |

---

## ğŸ¯ ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

```bash
# è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹
python pytorch_hypergraph_deep_learning.py
```

### è‡ªå®šä¹‰å‚æ•°

ä¿®æ”¹ä»£ç ä¸­çš„å‚æ•°ï¼š

```python
# åœ¨ main() å‡½æ•°ä¸­
fold_results, all_y_true, all_y_scores, fold_predictions = cross_validation(
    association_matrix=association_matrix,
    snorna_sim=snorna_sim,
    disease_sim=disease_sim,
    n_splits=5,        # äº¤å‰éªŒè¯æŠ˜æ•°
    epochs=100,        # è®­ç»ƒè½®æ•°
    lr=0.001,          # å­¦ä¹ ç‡
    patience=20        # æ—©åœè€å¿ƒå€¼
)

# åœ¨æ¨¡å‹åˆå§‹åŒ–ä¸­
model = DeepHypergraphNN(
    num_snorna=num_snorna,
    num_disease=num_disease,
    snorna_sim=snorna_sim,
    disease_sim=disease_sim,
    hidden_dims=[256, 128, 64],  # éšè—å±‚ç»´åº¦
    num_heads=8,                  # æ³¨æ„åŠ›å¤´æ•°
    dropout=0.3                   # Dropoutç‡
)
```

---

## ğŸ“Š é¢„æœŸç»“æœ

åŸºäºç›¸åŒçš„æ•°æ®é›†ï¼ŒPyTorchæ·±åº¦å­¦ä¹ æ¨¡å‹é¢„æœŸè¾¾åˆ°ï¼š

| æŒ‡æ ‡ | é¢„æœŸèŒƒå›´ | è¯´æ˜ |
|------|----------|------|
| **AUC** | 0.80 - 0.90 | é«˜äºä¼ ç»Ÿæ–¹æ³• |
| **AUPR** | 0.85 - 0.92 | ä¼˜ç§€çš„ç²¾ç¡®åº¦ |
| **è®­ç»ƒæ—¶é—´** | 10-30åˆ†é’Ÿ/æŠ˜ | å–å†³äºç¡¬ä»¶ |

### ä¸ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”

| æ–¹æ³• | AUC | AUPR | è®­ç»ƒæ—¶é—´ |
|------|-----|------|----------|
| ä¼ ç»Ÿè¶…å›¾æ‰©æ•£ | 0.7727 | 0.8305 | 2åˆ†é’Ÿ/æŠ˜ |
| **PyTorchæ·±åº¦å­¦ä¹ ** | **0.85+** | **0.88+** | **20åˆ†é’Ÿ/æŠ˜** |

---

## ğŸ¨ è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆåï¼Œå°†ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶ï¼ˆå‰ç¼€ `DL_`ï¼‰ï¼š

### å¯è§†åŒ–å›¾è¡¨
1. `DL_01_fold_comparison.png` - å„æŠ˜æ€§èƒ½å¯¹æ¯”
2. `DL_02_overall_roc_curve.png` - æ•´ä½“ROCæ›²çº¿
3. `DL_03_overall_pr_curve.png` - æ•´ä½“PRæ›²çº¿
4. `DL_04_all_folds_roc.png` - æ‰€æœ‰æŠ˜ROCæ±‡æ€»
5. `DL_05_all_folds_pr.png` - æ‰€æœ‰æŠ˜PRæ±‡æ€»
6. `DL_06_metrics_boxplot.png` - æ€§èƒ½ç®±çº¿å›¾
7. `DL_07_metrics_heatmap.png` - æ€§èƒ½çƒ­å›¾
8. `DL_08_comprehensive_panel.png` - ç»¼åˆé¢æ¿å›¾ â­æ¨è

### æ•°æ®æ–‡ä»¶
9. `DL_fold_results.csv` - å„æŠ˜è¯¦ç»†ç»“æœ
10. `DL_summary_statistics.csv` - ç»Ÿè®¡æ‘˜è¦

---

## ğŸ” æ¨¡å‹ä¼˜åŠ¿

### ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”

| ä¼˜åŠ¿ | ä¼ ç»Ÿæ–¹æ³• | PyTorchæ·±åº¦å­¦ä¹  |
|------|----------|----------------|
| **è¡¨è¾¾èƒ½åŠ›** | çº¿æ€§ | éçº¿æ€§ï¼Œå¤šå±‚æ¬¡ |
| **ç‰¹å¾å­¦ä¹ ** | æ‰‹å·¥ç‰¹å¾ | è‡ªåŠ¨å­¦ä¹  |
| **æ³¨æ„åŠ›æœºåˆ¶** | âŒ | âœ… å¤šå¤´æ³¨æ„åŠ› |
| **æ®‹å·®è¿æ¥** | âŒ | âœ… é˜²æ­¢æ¢¯åº¦æ¶ˆå¤± |
| **æ‰¹å½’ä¸€åŒ–** | âŒ | âœ… åŠ é€Ÿæ”¶æ•› |
| **GPUåŠ é€Ÿ** | âŒ | âœ… æ˜¾è‘—æé€Ÿ |
| **å¯æ‰©å±•æ€§** | æœ‰é™ | é«˜åº¦å¯æ‰©å±• |

### æŠ€æœ¯äº®ç‚¹

1. **ç«¯åˆ°ç«¯å­¦ä¹ **: ä»åŸå§‹ç›¸ä¼¼åº¦çŸ©é˜µç›´æ¥å­¦ä¹ é¢„æµ‹
2. **å¤šå°ºåº¦ç‰¹å¾**: é€šè¿‡å¤šå±‚ç½‘ç»œæ•è·ä¸åŒå±‚æ¬¡çš„ç‰¹å¾
3. **æ³¨æ„åŠ›æƒé‡**: è‡ªåŠ¨å­¦ä¹ é‡è¦çš„èŠ‚ç‚¹å’Œè¶…è¾¹
4. **æ­£åˆ™åŒ–æŠ€æœ¯**: Dropout + BatchNorm + Weight Decay
5. **è‡ªé€‚åº”ä¼˜åŒ–**: Adamä¼˜åŒ–å™¨ + å­¦ä¹ ç‡è°ƒåº¦

---

## âš™ï¸ è¶…å‚æ•°è°ƒä¼˜æŒ‡å—

### å­¦ä¹ ç‡
```python
# æ¨èèŒƒå›´: 0.0001 - 0.01
lr = 0.001  # é»˜è®¤å€¼ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ

# å¦‚æœè®­ç»ƒä¸ç¨³å®š
lr = 0.0005  # é™ä½å­¦ä¹ ç‡

# å¦‚æœæ”¶æ•›å¤ªæ…¢
lr = 0.003   # æé«˜å­¦ä¹ ç‡
```

### éšè—å±‚ç»´åº¦
```python
# å°æ•°æ®é›†
hidden_dims = [128, 64, 32]

# é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰
hidden_dims = [256, 128, 64]

# å¤§æ•°æ®é›†æˆ–è¿½æ±‚æ›´é«˜æ€§èƒ½
hidden_dims = [512, 256, 128, 64]
```

### Dropoutç‡
```python
# è¾ƒå°æ¨¡å‹æˆ–æ•°æ®å……è¶³
dropout = 0.2

# é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰
dropout = 0.3

# è¿‡æ‹Ÿåˆä¸¥é‡æ—¶
dropout = 0.5
```

### æ³¨æ„åŠ›å¤´æ•°
```python
# å¿…é¡»èƒ½æ•´é™¤éšè—å±‚ç»´åº¦
# è¾ƒå°æ¨¡å‹
num_heads = 4

# é»˜è®¤é…ç½®ï¼ˆæ¨èï¼‰
num_heads = 8

# è¿½æ±‚æ›´é«˜æ€§èƒ½
num_heads = 16  # æ³¨æ„: hidden_dimå¿…é¡»æ˜¯16çš„å€æ•°
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: CUDA out of memory é”™è¯¯

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. å‡å°æ‰¹å¤§å°ï¼ˆå¦‚æœä½¿ç”¨mini-batchï¼‰
# 2. å‡å°éšè—å±‚ç»´åº¦
hidden_dims = [128, 64, 32]

# 3. å‡å°‘æ³¨æ„åŠ›å¤´æ•°
num_heads = 4

# 4. ä½¿ç”¨CPUæ¨¡å¼
device = torch.device('cpu')
```

### Q2: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. ä½¿ç”¨GPU
device = torch.device('cuda')

# 2. å‡å°‘è®­ç»ƒè½®æ•°
epochs = 50

# 3. å¢å¤§å­¦ä¹ ç‡
lr = 0.003

# 4. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è¦GPUï¼‰
from torch.cuda.amp import autocast, GradScaler
```

### Q3: æ¨¡å‹ä¸æ”¶æ•›

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. é™ä½å­¦ä¹ ç‡
lr = 0.0005

# 2. å¢åŠ è®­ç»ƒè½®æ•°
epochs = 200

# 3. è°ƒæ•´æ—©åœè€å¿ƒå€¼
patience = 30

# 4. æ£€æŸ¥æ•°æ®å½’ä¸€åŒ–
# ç¡®ä¿ç›¸ä¼¼åº¦çŸ©é˜µåœ¨[0, 1]èŒƒå›´å†…
```

### Q4: æ€§èƒ½ä¸å¦‚é¢„æœŸ

**è§£å†³æ–¹æ¡ˆ:**
```python
# 1. å¢åŠ æ¨¡å‹å®¹é‡
hidden_dims = [512, 256, 128]

# 2. å‡å°dropoutç‡
dropout = 0.2

# 3. å¢åŠ æ³¨æ„åŠ›å¤´æ•°
num_heads = 16

# 4. å°è¯•ä¸åŒçš„kå€¼
k_snorna = 15
k_disease = 15
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®¡ç®—æ•ˆç‡

1. **ä½¿ç”¨GPU**: å¯æé€Ÿ5-10å€
```python
# æ£€æŸ¥GPUå¯ç”¨æ€§
if torch.cuda.is_available():
    device = torch.device('cuda')
    print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
```

2. **æ··åˆç²¾åº¦è®­ç»ƒ**: èŠ‚çœå†…å­˜å¹¶åŠ é€Ÿ
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

3. **æ•°æ®é¢„å¤„ç†**: æå‰è½¬æ¢ä¸ºtensor
```python
H_tensor = torch.FloatTensor(H).to(device)
```

### å†…å­˜ä¼˜åŒ–

1. **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹å¤§å°
```python
accumulation_steps = 4
for i, (input, target) in enumerate(dataloader):
    loss = model(input, target)
    loss = loss / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

2. **æ£€æŸ¥ç‚¹æœºåˆ¶**: èŠ‚çœå†…å­˜
```python
from torch.utils.checkpoint import checkpoint

# åœ¨æ¨¡å‹forwardä¸­ä½¿ç”¨
output = checkpoint(self.heavy_layer, input)
```

---

## ğŸ”¬ é«˜çº§åŠŸèƒ½

### 1. æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# ä¿å­˜æœ€ä½³æ¨¡å‹
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'auc': best_auc,
}, 'best_model.pth')

# åŠ è½½æ¨¡å‹
checkpoint = torch.load('best_model.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
```

### 2. TensorBoardå¯è§†åŒ–

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/hypergraph_experiment')

# è®°å½•æŸå¤±
writer.add_scalar('Loss/train', loss, epoch)

# è®°å½•æŒ‡æ ‡
writer.add_scalar('Metrics/AUC', auc, epoch)
writer.add_scalar('Metrics/AUPR', aupr, epoch)

# å¯è§†åŒ–æ¨¡å‹ç»“æ„
writer.add_graph(model, input_sample)

writer.close()

# åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir=runs
```

### 3. è¶…å‚æ•°æœç´¢

```python
import optuna

def objective(trial):
    # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)
    dropout = trial.suggest_uniform('dropout', 0.1, 0.5)
    hidden_dim = trial.suggest_categorical('hidden_dim', [128, 256, 512])
    
    # è®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯AUC
    model = create_model(lr, dropout, hidden_dim)
    auc = train_and_evaluate(model)
    
    return auc

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print(f"æœ€ä½³å‚æ•°: {study.best_params}")
print(f"æœ€ä½³AUC: {study.best_value}")
```

---

## ğŸ“š ä»£ç ç»“æ„

```
pytorch_hypergraph_deep_learning.py
â”‚
â”œâ”€â”€ æ•°æ®åŠ è½½
â”‚   â””â”€â”€ DataLoaderClass
â”‚
â”œâ”€â”€ è¶…å›¾æ„å»º
â”‚   â””â”€â”€ HypergraphConstructor
â”‚
â”œâ”€â”€ æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ HypergraphConvolution      # è¶…å›¾å·ç§¯å±‚
â”‚   â”œâ”€â”€ MultiHeadAttention         # å¤šå¤´æ³¨æ„åŠ›
â”‚   â”œâ”€â”€ HypergraphBlock            # è¶…å›¾å—
â”‚   â””â”€â”€ DeepHypergraphNN           # ä¸»æ¨¡å‹
â”‚
â”œâ”€â”€ è®­ç»ƒæµç¨‹
â”‚   â”œâ”€â”€ Trainer                    # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ train_epoch()              # å•è½®è®­ç»ƒ
â”‚   â””â”€â”€ evaluate()                 # æ¨¡å‹è¯„ä¼°
â”‚
â”œâ”€â”€ äº¤å‰éªŒè¯
â”‚   â””â”€â”€ cross_validation()         # KæŠ˜äº¤å‰éªŒè¯
â”‚
â”œâ”€â”€ å¯è§†åŒ–
â”‚   â””â”€â”€ ResultVisualizer           # ç»“æœå¯è§†åŒ–
â”‚
â””â”€â”€ ä¸»å‡½æ•°
    â””â”€â”€ main()
```

---

## ğŸ“ å¼•ç”¨ä¸å‚è€ƒ

å¦‚æœä½¿ç”¨æœ¬ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{hypergraph_snorna_prediction_2025,
  title={PyTorchæ·±åº¦è¶…å›¾ç¥ç»ç½‘ç»œ - snoRNA-Diseaseå…³è”é¢„æµ‹},
  author={Your Name},
  year={2025},
  note={åŸºäºPyTorchå®ç°çš„æ·±åº¦è¶…å›¾ç¥ç»ç½‘ç»œ}
}
```

### ç›¸å…³è®ºæ–‡

1. Feng et al. (2019) "Hypergraph Neural Networks"
2. Gao et al. (2020) "Hypergraph Learning: Methods and Practices"
3. Vaswani et al. (2017) "Attention Is All You Need"

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### é—®é¢˜åé¦ˆ
- æ£€æŸ¥ä»£ç æ³¨é‡Šè·å–è¯¦ç»†ä¿¡æ¯
- é˜…è¯»å¸¸è§é—®é¢˜éƒ¨åˆ†
- æ ¹æ®é”™è¯¯ä¿¡æ¯è°ƒæ•´å‚æ•°

### æ”¹è¿›å»ºè®®
- å¢åŠ æ›´å¤šå±‚æ•°æå‡æ€§èƒ½
- å°è¯•ä¸åŒçš„æ¿€æ´»å‡½æ•°
- é›†æˆå¤šä¸ªæ¨¡å‹è¿›è¡Œensemble

---

## ğŸ¯ ä¸‹ä¸€æ­¥è®¡åˆ’

- [ ] æ”¯æŒmini-batchè®­ç»ƒ
- [ ] æ·»åŠ å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)
- [ ] å®ç°æ¨¡å‹è§£é‡Šæ€§åˆ†æ
- [ ] æ”¯æŒæ›´å¤§è§„æ¨¡æ•°æ®é›†
- [ ] æ·»åŠ åœ¨çº¿å­¦ä¹ åŠŸèƒ½

---

**ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-23  
**çŠ¶æ€**: âœ… å®Œæ•´å®ç°ï¼Œå¯ç›´æ¥ä½¿ç”¨  
**è®¸å¯**: MIT License
